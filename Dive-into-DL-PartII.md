### Dive into deep learning - Part II

------

#### 过拟合欠拟合及其解决方案

训练误差（training error）和泛化误差（generalization error）。

预留一部分在训练数据集和测试数据集以外的数据来进行模型选择，被称为验证数据集（validation set）。

##### K折交叉验证（K-fold cross validation）

把原始训练数据集分割成K个不重合的子数据集，做K次模型训练和验证。每次使用一个子数据集验证模型，用其它K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，对K次训练误差和验证误差分别求平均。

##### 过拟合和欠拟合

欠拟合（underfitting）：模型无法得到较低的训练误差

过拟合（overfitting）：模型的训练误差远小于它在测试数据集上的误差

过拟合是指训练误差达到一个较低的水平，而泛化误差依然较大。
欠拟合是指训练误差和泛化误差都不能达到一个较低的水平。
发生欠拟合的时候在训练集上训练误差不能达到一个比较低的水平，所以过拟合和欠拟合不可能同时发生。

模型复杂度：随模型复杂度增加，训练误差越来越低；泛化误差呈U形，有最佳模型复杂度，对应泛化误差最低。

训练数据集：如果训练数据集样本数过少，特别是比模型参数数量更少时，过拟合容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许范围内，通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。

过拟合可以使用权重衰减和丢弃法来缓解，即使在一个比较小的数据集上使用了权重衰减和丢弃法之后也能够达到一个比较好的效果。

##### 权重衰减

权重衰减等价于$L_2$范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。

$L_2$范数惩罚项通过惩罚绝对值较大的参数的方法来应对过拟合。

##### 丢弃法

当对隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。丢弃法不改变输入的期望值。由于在训练中隐藏层神经元的丢弃是随机的，输出层的计算无法过度依赖$h_1,...h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。



#### 梯度消失、梯度爆炸

深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。

当神经网络的层数较多时，模型的数值稳定性容易变差。

梯度消失会导致模型训练困难，对参数的优化步长过小，收效甚微，模型收敛十分缓慢；

梯度爆炸会导致模型训练困难，对参数的优化步长过大，难以收敛。

sigmoid和tanh激活函数，会把元素转换到`[0,1]`和`[-1,1]`之间，会加剧梯度消失的现象。

错题总结：一个在冬季部署的物品推荐系统在夏季的物品推荐列表中出现了圣诞礼物，我们可以推断该系统没有考虑到：`协变量偏移`

解释：这个现象是由于协变量时间（或者说季节）发生了变化造成的。

##### Kaggle 房价预测实战

预处理数据：我们对连续数值的特征做标准化（standardization）：设该特征在整个数据集上的均值为$\mu$，标准差为$\sigma$。那么，我们可以将该特征的每个值先减去$\mu$再除以$\sigma$得到标准化后的每个特征值。对于缺失的特征值，我们将其替换成该特征的均值。

```python
loss = torch.nn.MSELoss()	# MSE,mean squared error,均方误差
# rmse, root mean squared error,均方根误差
# log_rmse,对数均方根误差
```

模型训练实战步骤：

1. 获取数据集
2. 数据预处理
3. 模型设计
4. 模型验证和模型调整（调参）
5. 模型预测以及提交

标签偏移可以简单理解为测试时出现了训练时没有的标签。

如果数据量足够的情况下，确保训练数据集和测试集中的数据取自同一个数据集，可以防止协变量偏移和标签偏移是正确的。如果数据量很小，小到测试集中存在训练集中未包含的标签，就会发生标签偏移。



#### 循环神经网络进阶

##### 门控循环神经网络（GRU）

GRU有重置门和更新门，重置门有助于捕捉时间序列里短期的依赖关系，更新门有助于捕捉时间序列里长期的依赖关系。

##### 长短期记忆（LSTM，long short term memory）

- 遗忘门：控制上一时间步的记忆细胞
- 输入门：控制当前时间步的输入
- 输出门：控制从记忆细胞到隐藏状态
- 记忆细胞：一种特殊的隐藏状态的信息流动

在LSTM模型的初始化中，需要初始化的参数包括：

- 第0个循环单元的记忆细胞和循环单元的值
- 门控单元中用于计算遗忘门的权重与偏差
- 用于计算输出的权重与偏差

而每个循环单元中的记忆细胞和循环单元的值为LSTM模型中的隐状态，而非参数，因此不需要初始化。



GRU和LSTM都能捕捉时间序列中时间步距离较大的依赖关系。

双向循环神经网络在文本任务里能做到同时考虑上文和下文与当前词之间的依赖。

LSTM和GRU能一定程度缓解梯度消失与梯度爆炸的问题。



#### 机器翻译和数据集

数据预处理中分词（Tokenization）的作用是：把字符形式的句子转化为单词组成的列表

把单词转化为词向量是模型结构的一部分，词向量层一般作为网络的第一层。

##### Sequence to Sequence模型

预测时decoder每个单元输出得到的单词作为下一个单元的输入单词，预测时decoder单元输出为句子结束符时跳出循环。每个batch训练时encoder和decoder都有固定长度的输入（每个batch的输入需要形状一致）。

##### Encoder-Decoder

encoder：输入到隐藏状态

decoder：隐藏状态到输出

Encoder-Decoder常应用于输入序列和输出序列的长度是可变的，如机器翻译、对话机器人、语音识别任务等，而分类问题（eg. 文本分类任务）的输出是固定的类别，不需要使用Encoder-Decoder。

##### 集束搜索（Beam Search）

集束搜索结合了greedy search和维特比算法，它是维特比算法的贪心形式。集束搜索使用beam size参数来限制在每一步保留下来的可能性词的数量。



#### 注意力机制和Seq2seq模型

错题总结1：

注意力机制借鉴了人类的注意力思维方式，以获得需要重点关注的目标区域。

在计算注意力权重时，在点积注意力（Dot-product Attention）中，key与query维度需要一致，在多层感知注意力（Multi-layer Perceptron Attention）中则不需要。

点积注意力层不引入新的模型参数。

注意力掩码（Attention Mask）可以用来解决一组变长序列的编码问题。

错题总结2:

seq2seq模型的预测需人为设定终止条件，设定最长序列长度或者输出[eos]结束符号，若不加以限制则可能生成无穷长度序列；

每个时间步，每个位置都会计算各自的attention输出，故解码器输入的语境向量（context vector）不同；

解码器RNN仍由编码器最后一个时间步的隐藏状态初始化；

注意力机制本身有高效的并行性（高维张量的矩阵乘法可用于并行计算多个位置的注意力分数），但引入注意力并不能改变seq2seq内部RNN的迭代机制，因此无法加速。

课后练习3: 关于点积注意力机制

计算点积后除以$\sqrt{d}$以减轻向量维度对注意力权重的影响；

可视化注意力权重的二维矩阵有助于分析序列内部的依赖关系；

对于两个有效长度不同的输入序列，即使两组键值对完全相同，但有效长度不同导致Attention Mask不同，屏蔽掉无效位置后进行attention，对于同一个query会导致不同的输出。



#### Transformer

主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）：

- CNNs易于并行化，却不适合捕捉变长序列内的依赖关系。
- RNNs适合捕捉长距离变长序列的依赖，但却难以实现并行化处理序列。

为了整合CNN和RNN的优势，`Vaswani et al., 2017`创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。

Transformer与seq2seq模型相似，同样基于Encoder-Decoder架构，其区别主要在于：

1. Transformer blocks：将seq2seq模型中的循环网络替换为Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。
2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化（layer norm）。
   - layer norm与batch norm相似，唯一区别在于batch norm是对于batch size这个维度进行计算均值和方差的，而layer norm则是对最后一维进行计算（层归一化对一个中间层的所有神经元进行归一化）；
   - layer norm可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能
   - 注意区分：批归一化（batch normalization）对每个神经元的输入数据以mini-batch为单位进行汇总
3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。

##### 多头注意力层

多头注意力层包含$h$个并行的自注意力层，每一个这种层被称为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个线性层进行映射，这$h$个注意力头的输出将会被拼接（concatenate）之后输入最后一个线性层进行整合。

##### 基于位置的前馈网络

Transformer模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为(batch_size, seq_length, feature_size)的三维张量。Position-wise FFN由两个全连接层组成，它们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以称为position-wise，这等效于一个$1\times1$的卷积。

##### 解码器

Transformer模型的解码器有一个多头注意力子模块，接受编码器的输出作为key和value，decoder的状态作为query（所以不是“自注意力”模块）。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。

仔细来讲，在第$t$个时间步，当前输入$x_t$是query，那么self attention接受了第$t$步以及前$t-1$步的所有输入$x_1,...,x_{t-1}$。在训练时，由于第$t$位置的输入可以观测到全部的序列，这与预测阶段的情形相矛盾，所以我们要通过将第$t$个时间步所对应的可观测长度设置为$t$，以消除不需要看到的未来的信息。

错题总结：

关于Transformer的描述：

- 在训练过程中，解码器部分只需进行一次前向传播；在预测过程中，解码器部分要进行句子长度（seq_length）次前向传播
- 解码器部分在预测过程中不需要使用Attention Mask
- 自注意力会计算句子内任意两个位置的注意力权重，因此自注意力模块理论上可以捕捉任意距离的依赖关系

在Transformer模型中，注意力头数为$h$，嵌入向量和隐藏状态维度均为$d$，那么一个多头注意力层所含的参数量是： $4hd^2$

$h$个注意力头中，每个参数量为$3d^2$，最后的输出层形状为$hd\times d$，所以参数量共为$4hd^2$。



#### 卷积神经网络基础

##### 二维互相关运算

二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。

##### 二维卷积层

二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。

##### 互相关运算与卷积运算

卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。

##### 特征图与感受野

二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。

##### 填充和步幅

填充（padding）是指在输入高和宽的两侧填充元素（通常是0）。

在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。

##### 多输入通道和多输出通道

之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3\times h\times w$的多维数组，我们将大小为3的这一维称为通道（channel）维。

##### 卷积层与全连接层的对比

二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：

1. 全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。
2. 卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i,c_o,h,w)$的卷积核的参数量是$c_i\times c_o\times h\times w$，与输入图像的宽高无关。假如一个卷积层的输入和输出形状分别是$(c_1,h_1,w_1)$和$(c_2,h_2,w_2)$，如果要用全连接层进行连接，参数数量就是$c_1\times c_2\times h_1\times w_1\times h_2\times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。
   - $c_i$		 # input channels
   - $c_o$        # output channels

##### 二维池化层

池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。

池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。

在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。

错题总结：

1. 假如你用全连接层处理一张$256\times256$的彩色（RGB）图像，输出包含$1000$个神经元，在使用偏置的情况下，参数数量是：

   $3\times256\times256\times1000+1000=196609000$

   图像展平后长度为$3\times256\times256$，每个神经元对应一个偏置参数

2. 假如你用全连接层处理一张$256\times256$的彩色（RGB）图像，卷积核的高宽是$3\times3$，输出包含$10$个通道，在使用偏置的情况下，这个卷积层共有多少个参数：

   $c_i\times c_o\times h\times w+bias=3\times10\times3\times3+10=280$

   别忘记输入通道（彩色3 channels），每个输出chaneel对应一个偏置参数

3. `conv2d = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=2)`，输入一张形状为$3\times100\times100$的图像，输出的形状为：

   🤔️一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：
   $$
   \lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor\times\lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor
   $$
   n - input		p - padding		k - kernel		s - stride

   输出通道数为4，输出形状为$102\times102\hspace{2mm}(100+4-3+1=102)$

   注意上下两侧总共填充4行，同理，左右两侧总共填充4行。

4. 关于卷积层：

   - $1\times1$卷积可以看作是通道维上的全连接
   - 对于高宽维度，只要输入的高宽（填充后的）$\ge$  卷积核的高宽即可进行计算
   - 卷积层通过填充、步幅、输入通道数、输出通道数等调节输出的形状
   - 两个连续的$3\times3$卷积核的感受野与一个$5\times5$卷积核的感受野相同

5. 关于池化层：

   - 池化层有参与模型的正向计算，同样也参与反向传播
   - 池化层直接对窗口内的元素求最大值或平均值，并没有模型参数参与计算
   - 池化层通常会减小特征图的高和宽
   - 池化层的输入和输出具有相同的通道数



#### LeNet

LeNet分为卷积层块和全连接层块两部分。卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。

卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5\times5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。

LeNet在连接卷积层块和全连接层块时，需要做一次展平操作。

全连接层块含3个全连接层。LeNet模型中，90%以上的参数集中在全连接层块。

LeNet交替使用卷积层和池化层后接全连接层（dense）来进行图像分类。

卷积神经网络通过使用滑动窗口在输入的不同位置处重复计算，减小参数数量。

在通过卷积层或池化层后，输出的高和宽可能减小，为了尽可能保留输入的特征，我们可以在减小高宽的同时增加通道数。



#### 卷积神经网络进阶

##### 深度卷积神经网络（AlexNet）

首次证明了学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的现状。

特点：

1. 8层变换，包括5层卷积、2层全连接隐藏层，及1个全连接输出层
2. 将sigmoid激活函数改成了更简单的ReLU激活函数
3. 用Dropout来控制全连接层的模型复杂度
4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

##### 使用重复元素的网络（VGG）

VGG：通过重复使用简单的基础块来构建深度模型。

Block：数个相同的padding为1、窗口形状为$3\times3$的卷积层，接上一个步幅为2、窗口形状为$2\times2$的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

##### 网络中的网络（NiN）

LeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。

NiN：串联多个由卷积层和“全连接”层（$1\times1$卷积层）构成的小网络来构建一个深层网络

用了输出通道数等于标签类别数的NiN块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。

$1\times1$卷积核作用：

- 放缩通道数：通过控制卷积核对数量达到通道数的放缩
- 增加非线性：$1\times1$卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性
- 计算参数少

NiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。
NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。
NiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。

##### GoogLeNet

1. 由Inception基础块组成
2. Inception块相当于一个有4条线路的子网络，它通过不同窗口形状的卷积层和最大池化层来并行抽取信息，并使用$1\times1$卷积层减少通道数从而降低模型复杂度
3. 可以自定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。



习题总结：

1.  通道数为3，宽高均为224的输入，经过一层输出通道数为96，卷积核大小为11，步长为4，无padding的卷积层后，得到的feature map的宽高为

   ​		
   $$
   \lfloor(224-11)/4\rfloor+1=54
   $$

2. NiN使用全局平均池化层对每个通道中所有元素求平均并直接用于分类；

   GoogLeNet通过不同窗口形状的卷积层和最大池化层来并行抽取信息；

   与AlexNet相比，VGG可以灵活地改变模型结构，网络表达能力更强；

   VGG通过重复使用简单的基础块来构建深度模型。



