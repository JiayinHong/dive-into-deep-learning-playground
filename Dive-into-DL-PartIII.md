### Dive into deep learning - Part III

------

#### 批量归一化和残差网络

##### 批量归一化（BatchNormalization）

对输入的标准化（浅层模型）：处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近

批量归一化（深度模型）：利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。

1. 对全连接层做批量归一化
   - 位置：全连接层中的仿射变换和激活函数之间。
   - 引入可学习参数：拉伸参数$\gamma$和偏移参数$\beta$（所以它们不是超参数）。
2. 对卷积层做批量归一化
   - 位置：卷积计算之后、应用激活函数之前。
   - 如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。
   - 计算：对单通道，$batchsize=m$，卷积计算输出$=p\times q$，对该通道中$m\times p\times q$个元素同时做批量归一化，使用相同的均值和方差。
3. 预测时的批量归一化
   - 训练：以batch为单位，对每个batch计算均值和方差。
   - 预测：用移动平均估算整个训练数据集的样本均值和方差。

##### 残差网络（ResNet）

深度学习的问题：深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，准确率也变得更差。

残差块（Residual Block）

##### 稠密连接网络（DenseNet）

ResNet与DenseNet在跨层连接上的主要区别：ResNet使用“相加”，DenseNet使用“连结”（concatenation）

主要构件模块：

- 稠密块（dense block）：定义了输入和输出是如何连结的。
- 过渡层（transition layer）：用来控制通道数，使之不过大。
  - $1\times1$卷积层：减小通道数
  - 步幅为2的平均池化层：减半高和宽

习题总结：

1. `nn.BatchNorm2d()`表示卷积层的BN（BatchNormalization），参数为通道数。`nn.BatchNorm1d()`表示全连接层的BN，参数为输出神经元个数。
2. 在稠密块中，假设由3个输出通道数为8的卷积层组成，稠密块的输入通道数是3，那么稠密块的输出通道数是：
   - `输出通道数=输入通道数+卷积层个数*卷积输出通道数`
   - 输出通道数$=3+3*8=27$



#### 凸优化

##### 优化与估计

尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。

- 优化方法目标：训练集损失函数值
- 深度学习目标：测试集损失函数值（泛化性）

优化在深度学习中的挑战：

1. 局部最小值
2. 鞍点
   - 鞍点是对所有自变量一阶偏导数都为0，且Hessian矩阵特征值有正有负的点
3. 梯度消失

##### 凸性（Convexity）

Jensen不等式
$$
\sum_i{\alpha_if(x_i)}\ge f\left(\sum_i{\alpha_i x_i}\right)\hspace{2mm}and\hspace{2mm}E_x[f(x)]\ge f(E_x[x])
$$
“函数值的期望大于期望的函数值”

凸函数的性质：

1. 无局部极小值
2. 与凸集的关系
3. 二阶条件

习题总结：

1. 假设A和B都是凸集合，那么$A\cap B$一定是凸集合，而$A\cup B$不一定是凸集合。
2. 有限制条件的优化问题可以用以下方法来解决：
   - 拉格朗日乘子法
   - 添加惩罚项
   - 投影法



#### 梯度下降

##### 一维梯度下降

沿梯度反方向移动自变量可以减小函数值

学习率：沿着梯度反方向移动的距离

- 学习率过小，会导致收敛慢；

- 学习率过大（如大于1），会导致二阶项不可忽略，可能发生梯度上升，导致发散

##### 多维梯度下降

##### 自适应方法（自动调整学习率或不需要设置学习率）

- 牛顿法
  - 牛顿法相比梯度下降的一个优势在于：梯度下降“步幅”的确定比较困难，而牛顿法相当于可以通过Hessian矩阵来调整“步幅”
  - 牛顿法需要计算Hessian矩阵的逆，计算量比较大
  - 在牛顿法中，局部极小值也可以通过调整学习率来解决
- 收敛性分析
- 预处理（Heissan阵辅助梯度下降）
- 梯度下降与线性搜索（共轭梯度法）

##### 随机梯度下降

- 对于有$n$个样本的训练数据集，梯度下降一次参数更新的时间复杂度为$\mathcal{O}(n)$
- 而随机梯度下降一次参数更新的时间复杂度为$\mathcal{O}(1)$

##### 动态学习率

- 在最开始学习率设计比较大，加速收敛
- 学习率可以设计为指数衰减或多项式衰减
- 在优化进行一段时间后可以适当减小学习率来避免振荡

##### 小批量随机梯度下降

​	小批量随机梯度下降（选取一部分样本的平均梯度进行参数更新）是梯度下降（使用整个数据集上所有样本的平均梯度进行参数更新）和随机梯度下降（使用单个样本的梯度对参数进行更新）的折中。

梯度下降、随机梯度下降和小批量随机梯度下降的区别在于每次更新时用的样本量，可以通过修改`train_sgd`函数的参数`batch_size`来切换这三种模式



#### 优化算法进阶

##### Momentum

目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient，我们需要谨慎地选取学习率和batch size，来控制梯度方差和收敛的结果。

##### An ill-conditioned problem

Condition number of Hessian Matrix:
$$
cond_H=\frac{\lambda_{max}}{\lambda_{min}}
$$
where $\lambda_{max}$，$\lambda_{min}$ is the maximum and minimum eigenvalue  of Hessian Matrix.

##### Maximum Learning Rate

- For $f(x)$, according to convex optimization conclusions, we need step size (learning rate) $\eta < \frac{1}{L}$ to have the fastest convergence, where $L=\max_x\nabla^2f(x)$ called smooth.
- To guarantee the convergence, we need to have $\eta < \frac{2}{L}$

##### Preconditioning

在二阶优化中，我们使用Hessian matrix的逆矩阵（或者pseudo inverse）来左乘梯度向量 $i.e.\Delta_x=H^{-1}\mathbf{g}$，这样的做法称为precondition，相当于将$H$映射为一个单位矩阵，拥有分布均匀的spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。

##### Solution to ill-condition

- **Preconditioning gradient vector**: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and  other second-order optimization algorithms
- **Averaging history gradient**: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum

##### Momentum Algorithm（动量法）

当$x_1$和$x_2$的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均(exponential moving average)使得自变量的更新方向更加一致，从而降低发散的可能。

##### Exponential Moving Average (指数移动平均)

给定超参数$0\le\beta<1$，当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合：
$$
y_t=\beta y_{t-1}+(1-\beta)x_t
$$
对$y_t$展开：
$$
\begin{align*}
y_t&=(1-\beta)x_t+\beta y_{t-1}\\
&=(1-\beta)x_t+(1-\beta)\cdot \beta x_{t-1}+\beta^2y_{t-2}\\
&=(1-\beta)x_t+(1-\beta)\cdot\beta x_{t-1}+(1-\beta)\cdot\beta^2x_{t-2}+\beta^3y_{t-3}\\
&=(1-\beta)\sum_{i=0}^t{\beta^i x_{t-i}}
\end{align*}
$$
所谓“指数移动平均”👆
$$
(1-\beta)\sum_{i=0}^t{\beta^i}=\frac{1-\beta^t}{1-\beta}(1-\beta)=1-\beta^t
$$
只有当$t\to\infty$时，权重和才为1，才是无偏加权

##### AdaGrad

AdaGrad算法根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。

AdaGrad算法会使用一个小批量随机梯度$\mathbf{g}_t$按元素平方的累加变量$\mathbf{s}_t$。在时间步0，AdaGrad将$\mathbf{s}_0$中每个元素初始化为0.在时间步$t$，首先将小批量随机梯度$\mathbf{g}_t$按元素平方后累加到变量$\mathbf{s}_t$：
$$
\mathbf{s}_t\leftarrow\mathbf{s}_{t-1}+\mathbf{g}_t\odot\mathbf{g}_t
$$
其中$\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：
$$
\mathbf{x}_t\leftarrow\mathbf{x}_{t-1}-\frac{\eta}{\sqrt{\mathbf{s}_t+\epsilon}}\odot\mathbf{g}_t
$$
其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。

需要强调的是，小批量随机梯度按元素平方的累加变量$\mathbf{s}_t$出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$\mathbf{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。

##### RMSProp

不同于AdaGrad算法里状态变量$\mathbf{s}_t$是截至时间步$t$所有小批量随机梯度$\mathbf{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，计算
$$
\mathbf{v}_t\leftarrow\beta\mathbf{v}_{t-1}+(1-\beta)\mathbf{g}_t\odot\mathbf{g}_t
$$
和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素都学习率通过按元素运算重新调整，然后更新自变量
$$
\mathbf{x}_t\leftarrow\mathbf{x}_{t-1}-\frac{\alpha}{\sqrt{\mathbf{v}_t+\epsilon}}\odot\mathbf{g}_t
$$
其中$\alpha$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\mathbf{v}_t$是对平方项$\mathbf{g}_t\odot\mathbf{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\beta)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。

举个例子，将初始学习率$\alpha$设为0.01，并将超参数$\beta$设为0.9。此时，变量$\mathbf{v}_t$可看作是最近$1/(1-0.9)=10$个时间步的平方项$\mathbf{g}_t\odot\mathbf{g}_t$的加权平均。

##### AdaDelta

除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进。有意思的是，AdaDelta算法并没有学习率这一超参数。

AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\mathbf{g}_t$按元素平方的指数加权移动平均变量$\mathbf{s}_t$。在时间步0，它的所有元素被初始化为0。与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\Delta\mathbf{x}_t$，其元素同样在时间步0时被初始化为0。如不考虑$\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\sqrt{\Delta\mathbf{x}_{t-1}}$来替代超参数学习率。

##### Adam

Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。

Adam算法使用了动量变量$\mathbf{m}_t$（即小批量随机梯度$\mathbf{g}_t$的指数加权移动平均）和RMSProp算法中小批量随机梯度**按元素平方**的指数加权移动平均变量$\mathbf{v}_t$

需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。为了消除这样的影响，Adam算法中，对变量$\mathbf{m}_t$和$\mathbf{v}_t$均作偏差修正

##### 本节介绍的算法总结

- SGD Momentum
  - 对梯度计算Exponential Moving Average
- AdaGrad
  - 其自适应学习率没有使用EMA，而是对梯度平方进行累加（Preconditioning思想），因而存在梯度消失的问题
  - 出现梯度消失的原因是自适应学习率分母的不断累加使其存在最终趋于0的可能
- RMSProp
  - 其自适应学习率分母使用了EMA（利用EMA对上一时刻的自适应学习率的分母进行衰减
  - 不是直接对梯度平方进行累加，利用Exponential Moving Average解决了AdaGrad梯度消失的问题
- AdaDelta
  - 基于RMSProp的改进算法，只需传入EMA的衰减参数（只有一个超参数
- Adam
  - 自适应学习率的分子（$\hat{\mathbf{m}_t}$，梯度一阶矩的无偏估计）和分母（$\sqrt{\hat{\mathbf{v}_t}}$，梯度二阶矩无偏估计的根式）都使用了EMA
  - Adam使用了两次Exponential Moving Average，但二者的衰减参数并不相同
  - 是RMSProp与Momentum算法的结合，并对EMA权重进行了无偏操作（偏差修正
  - 对大小相差很大数量级的梯度都可以rescale到相近的大小（$\mathbf{m}_t$和$\mathbf{v}_t$分别是梯度的一阶矩和二阶矩估计，二者作商，可以使更新量rescale到1的附近



#### Word2Vec

##### 词嵌入基础

前面介绍用one-hot向量表示单词，虽然构造容易，但缺点是无法准确表达不同词之间的相似度

Word2Vec词嵌入工具将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。

基于两种概率模型的假设，可以定义两种Word2Vec模型：

1. Skip-Gram跳字模型：假设背景词由中心词生成，即建模$P(w_o|w_c)$，其中$w_c$为中心词，$w_o$为任一背景词
2. CBOW（continuous bag-of-words）连续词袋模型：假设中心词由背景词生成，即建模$P(w_c|W_o)$，其中$W_o$为背景词的集合

##### PTB数据集

简单来说，Word2Vec能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。那么为了训练Word2Vec模型，我们就需要一个自然语言语料库，模型将从中学习各个单词间的关系，这里我们使用PTB语料库进行训练。PTB（Penn Tree Bank）是一个常用的小型语料库，采样自《华尔街日报》的文章，包括训练集、验证集和测试集。

##### 二次采样

文本数据中一般会出现一些高频词，如英文中的"the","a"和"in"。通常来说，在一个背景窗口中，一个词（如“chip”）和低频词（如“microprocessor”）同时出现，比和高频词（如“the”）同时出现对训练词嵌入模型更有益。因此，训练词嵌入模型时可以对词进行二次采样。即数据集中每个被索引词将有一定概率被丢弃，且越高频的词被丢弃的概率越大。

##### Skip-Gram跳字模型

每个词被表示成两个$d$维向量，用来计算条件概率。

##### 负采样近似（negative sampling）

##### PyTorch预置的批量乘法

```python
X = torch.ones((2,1,4))		# tensor第一维维度需相等
Y = torch.ones((2,4,6))		# tensor后两维作矩阵乘法
print(torch.bmm(X,Y).shape)
```

torch.Size([2,1,6])

习题总结：

- 词嵌入模型与one-hot向量的比较：
  - one-hot向量只是一个简单的编码，很难包含复杂的语义信息如词语的相似性等；而训练好的词向量则可以从向量的空间关系上去体现词语间的关系，从而蕴含一定的语义信息
  - 词向量的维度是可以自由设定的：用one-hot向量表示词语时，为了使每个词语都获得唯一的编码，向量长度至少要与词典大小相当；而词嵌入模型中的词向量维度则没有这个限制（实际上，词嵌入可以看作是对one-hot词向量基于语义相似度进行的一个降维操作
  - 词嵌入模型首先需要在大规模语料库上进行训练，才能得到更有意义的词向量，其次在后续模型的训练过程中，可能还需要进一步的模型参数优化，所以在实现和使用上，都是比one-hot向量更复杂的
  - 无论是Skip-Gram模型还是CBOW模型，都是假设词语的含义是由其周围的单词所决定的。而为了使模型能够“学会”词语的含义，就必须将其置于大规模语料库上进行长时间的训练
- 对定义好的 Embedding 层 `embed = nn.Embedding(num_embedding=5, embed_dim=10)` 进行前向计算操作 `x = embed(torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.long))`，得到的张量形状是：$3\times2\times10$
  - `nn.Embedding` 层的实际作用就是将整数张量中的下标，替换为词向量，从张量形状上看，就是在最后添加 `embed_dim` 维，故得到的张量形状是 $3\times 2\times10$，代码中 `num_embedding` 为词典的大小。
- 在大语料库上进行大规模的词向量训练时：
  - 需要在训练时使用负采样近似，即对每个中心词都采集若干噪音词。这是因为大语料库意味着大的词典，若不使用负采样近似方法，词嵌入模型进行前向计算和梯度回传时，`softmax`的计算代价将是难以承受的
  - 需要分别定义中心词和背景词的词嵌入层。这是因为模型假设中心词和背景词都处于一种非对称的关系，而模型的数学表达式里，向量的点积项又是对称的，所以只能通过引入两个词嵌入层来保留假设中的非对称关系
  - 需要在词典中去掉出现频率极低的词，或将其在文本中替换为`<unk>`等特殊字符。这是因为大语料库中通常含有非常多的低频词，若不对其进行处理，将会严重损害模型的泛化能力，甚至降低高频词词向量的质量；同时，更大的词典也意味着更大的存储和计算开销
  - 词嵌入本质上就是在对词典进行降维操作，所以过大的词向量维度，反而可能会导致模型过拟合



#### 词嵌入进阶

 Word2Vec 词嵌入模型通过词向量的余弦相似度搜索近义词。虽然 Word2Vec 已经能够成功地将离散的单词转换为连续的词向量，并能一定程度上地保存词与词之间的近似关系，但 Word2Vec 模型仍不是完美的，它还可以被进一步地改进：

1. 子词嵌入（subword embedding）：[FastText](https://zh.d2l.ai/chapter_natural-language-processing/fasttext.html) 以固定大小的 n-gram 形式将单词更细致地表示为了子词的集合，而 [BPE (byte pair encoding)](https://d2l.ai/chapter_natural-language-processing/subword-embedding.html#byte-pair-encoding) 算法则能根据语料库的统计信息，自动且动态地生成高频子词的集合；
2. [GloVe 全局向量的词嵌入](https://zh.d2l.ai/chapter_natural-language-processing/glove.html): 通过等价转换 Word2Vec 模型的条件概率公式，我们可以得到一个全局的损失函数表达，并在此基础上进一步优化模型。

实际中，我们常常在大规模的语料上训练这些词嵌入模型，并将预训练得到的词向量应用到下游的自然语言处理任务中。本节就将以 GloVe 模型为例，演示如何用预训练好的词向量来求近义词和类比词。

##### GloVe全局向量的词嵌入

##### 求近义词和类比词（analogy）

我们可以通过寻找空间中的k近邻，来查询单词的近义词。

除了求近义词以外，我们还可以使用预训练词向量求词与词之间的类比关系，例如"man"之于"woman"相当于"son"之于"daughter"。求类比词问题可以定义为：对于类比关系中的4个词“$a$之于$b$相当于$c$之于$d$“，给定前3个词，求$d$。求类比词的思路是，搜索与$vec(c)+vec(b)-vec(a)$的结果向量最相似的词向量。

习题总结：

1. 对于Skip-Gram，CBOW，GloVe等词嵌入方法的理解：
   - 词嵌入模型的训练本质上是在优化模型预测各词语同时出现的概率
   - 词嵌入方法都是通过在大规模的语料库上进行训练，来让模型更好地“理解”词义，而好的模型设计则能提高训练的效率及模型的上界
   - 由于他人训练词向量时用到的语料库和当前任务上的语料库通常都不相同，所以词典中包含的词语以及词语的顺序都可能有很大差别，此时应当根据当前数据集上词典的顺序，来依次读入词向量，同时，为了避免训练好的词向量在训练的最初被破坏，还可以适当调整嵌入层的学习速率甚至设定其不参与梯度下降
   - GloVe模型用到了语料库上全局的统计信息，而Skip-Gram和CBOW模型则只用到了局部的统计信息
2. 关于GloVe方法基于Skip-Gram的改动：
   - GloVe使用了非概率分布的变量，并添加了中心词和背景词的偏差项，这样做是在松弛概率的规范性，即各个概率事件的概率和加起来等于1
   - GloVe使用了一个单调递增的权重函数来加权各个损失项
   - GloVe用平方损失函数替代了交叉熵损失函数
   - GloVe的损失函数计算公式用到了语料库上的全局统计信息
3. 关于利用词向量求近义词和类比词：
   - 由于计算资源和时间都很有限，我们通常会加载他人预训练好的词向量，而非在大规模语料库上从头开始训练
   - 在进行预训练词向量的载入时，我们需要根据任务的特性来选定语料库的大小和词向量的维度，以均衡模型的表达能力和泛化能力，同时还要兼顾计算的时间复杂度
   - 词语含义上的相似性和词向量空间中的余弦相似性是可以对应的
   - 求类比词时可以复用求近义词的代码：求类比词时我们先会对给定的三个词的词向量进行加减运算，以得到一个虚拟的词向量，再去求这个虚拟词向量的近义词，就可以找到类比词



#### 文本分类

##### 文本情感分类

文本分类是自然语言处理的一个常见任务，它把一段不定长的文本序列变换为文本的类别。本节关注它的一个子问题：使用文本情感分类来分析文本作者的情绪。这个问题也叫情感分析，并有着广泛的应用。

同搜索近义词和类比词一样，文本分类也属于词嵌入的下游应用。在本节中，我们将应用训练的词向量和含多个隐藏层的双向循环神经网络与卷积神经网络，来判断一段不定长的文本序列中包含的是正面还是负面的情绪。

##### 一维卷积层

在介绍模型前我们先来解释一维卷积层的工作原理。与二维卷积层一样，一维卷积层使用一维的互相关运算。在一维互相关运算中，卷积窗口从输入数组的最左方开始，按从左往右的顺序，依次在输入数组上滑动。当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。例如，输入是一个宽为 7 的一维数组，核数组的宽为 2，那么输出的宽度为 7−2+1=6。

多输入通道的一维互相关运算也与多输入通道的二维互相关运算类似：在每个通道上，将核与相应的输入做一维互相关运算，并将通道之间的结果相加得到输出结果。

由二维互相关运算的定义可知，多输入通道的一维互相关运算可以看作单输入通道的二维互相关运算。

我们也可以在一维卷积层指定多个输出通道，从而拓展卷积层中的模型参数。

##### 时序最大池化层

类似地，我们有一维池化层。TextCNN 中使用的时序最大池化（max-over-time pooling）层实际上对应一维全局最大池化层：假设输入包含多个通道，各通道由不同时间步上的数值组成，各通道的输出即该通道所有时间步中最大的数值。因此，时序最大池化层的输入在各个通道上的时间步数可以不同。

为提升计算性能，我们常常将不同长度的时序样本组成一个小批量，并通过在较短序列后附加特殊字符（如0）令批量中各时序样本长度相同（padding）。这些人为添加的特殊字符当然是无意义的。由于时序最大池化的主要目的是抓取时序中最重要的特征，它通常能使模型不受人为添加字符的影响。

##### TextCNN模型

TextCNN模型主要使用了一维卷积层和时序最大池化层。其计算主要分为以下几步：

1. 定义多个一维卷积核，并使用这些卷积核对输入分别做卷积计算。宽度不同的卷积核可能会捕捉到不同个数的相邻词的相关性。
2. 对输出的所有通道分别做时序最大池化，再将这些通道的池化输出值连结为向量。
3. 通过全连接层将连结后的向量变换为有关各类别的输出。这一步可以使用丢弃层应对过拟合。

习题总结：

1. 关于数据的读取：
   - 除了运用一个固定的分隔符进行单词的切分外，在实现分词函数时，还能加上更多的功能，如子词的切分和特殊字符的处理等。针对任务或语言的特性特殊实现的分词函数，将更有利于模型的训练，但在设计该函数时要注意预训练词向量是定义在什么词典上的，不能使out-of-vocabulary词过多
   - 无论是循环神经网络，还是卷积神经网络，理论上都是能在任何长度的文本上进行运算的，但规范化模型的长度（对数据中的句子进行补齐或截断等）会加强模型并行计算的能力，即有利于模型进行批量化的计算
   - （将文本序列转化为下标张量时）PyTorch的嵌入层在进行前向传播时，其输入张量需要为`torch.long`格式
2. 关于使用双向循环神经网络进行文本情感分类：
   - 由于自然语言中常常有一些特殊的倒装结构，所以拥有两个方向的隐藏状态的双向循环神经网络，其输出更具文本代表性。注意，虽然两个方向的隐藏状态在输出前被拼接在了一起，但他们仍然是独立地被运算出来的，所以双向循环神经网络也不能从根本上解决文本的双向依赖问题，而卷积神经网络和 Transformer 这样完全并行的结构则不存在该问题
   - 对于每个方向的每个隐藏状态，计算它时都会用到该方向中上一步的隐藏状态，所以双向循环神经网络中的隐藏状态不能被并行地算出
   - 通常会给循环神经网络模型输入两个特殊的初始状态向量，以表示文本的两端
   - 双向循环神经网络可以进行多层叠加：将双向循环神经网络的输出再次作为输入，输入到下一层的双向循环神经网络中，从而得到一个多层的结构
3. 关于使用卷积神经网络进行文本情感分类
   - 一维卷积输出的宽度为输入的宽度减去核的宽度加一
   - 定义多个输出通道的卷积核有利于模型提取更丰富的文本特征：输出通道数越多，可以捕捉的单词组合就越多；定义多种宽度的卷积核有利于模型提取多个层次的文本特征：拥有不同宽度的核，就能让模型对文本中各个长度的单词组合都有关注
   - 并不需要对每一个卷积核都单独地定义一个池化层：由于池化操作与输入的序列长度无关，本身也不含任何参数，故可以所有卷积核共用一个池化层
   - 对所有卷积操作的结果进行池化和拼接之后，得到的向量就可以作为文本的一个整体的表示：每个卷积和池化后的结果都可以看作是该卷积核在文本上提取出的特征，而拼接这些特征，就能得到整个文本的一个整体表示



#### 数据增强

##### 图像增广（image augmentation）

图像增广技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。我们也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。可以说，在当年AlexNet的成功中，图像增广技术功不可没。本节我们将讨论这个在计算机视觉里被广泛使用的技术。

##### 常用的图像增广方法

1. 翻转和裁剪

```python
apply(img, torchvision.transforms.RandomHorizontalFilp())		# 左右翻转
apply(img, torchvision.transforms.RandomVerticalFlip())		# 上下翻转
```

​	池化层能降低卷积层对目标位置的敏感度。除此之外，我们还可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。

​	在下面的代码里，我们每次随机裁剪出一块面积为原面积10%∼100%的区域，且该区域的宽和高之比随机取自0.5∼2，然后再将该区域的宽和高分别缩放到200像素。若无特殊说明，本节中a和b之间的随机数指的是从区间[a,b]中随机均匀采样所得到的连续值。

```python
shape_aug = torchvision.transforms.RandomResizedCrop(200, scale=(0.1,1), ratio=(0.5,2))
apply(img, shape_aug)
```

2. 变化颜色

   另一类增广方法是变化颜色。我们可以从4个方面改变图像的颜色：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）。在下面的例子力，我们将图像的亮度随机变化为原图亮度的50% (1-0.5) ~ 150% (1+0.5)。

   ```python
   apply(img, torchvision.transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0))
   ```

   我们也可以随机变化图像的色调。

   ```python
   apply(img, torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5))
   ```

   类似地，我们也可以随机变化图像的对比度。

   ```python
   apply(img, torchvision.transforms.ColorJitter(brightness=0, contrast=0.5, saturation=0, hue=0))
   ```

   我们也可以同时设置如何随机变化图像的亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue）。

   ```python
   color_aug = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)
   apply(image, color_aug)
   ```

3. 叠加多个图像增广方法

   实际应用中我们会将多个图像增广方法叠加使用。我们可以通过Compose实例将上面定义的多个图像增广方法叠加起来，再应用到每张图像之上。

   ```python
   augs = torchvision.transforms.Compose([torchvision.transforms.RandomHorizontalFlip(), color_aug, shage_aug])
   apply(img, augs)
   ```

##### 使用图像增广训练模型

为了在预测时得到确定的结果，我们通常只将图像增广应用在训练样本上，而不在预测时使用含随机操作的图像增广。在这里我们只使用最简单的随机左右翻转。此外，我们使用ToTensor将小批量图像转成PyTorch需要的格式，即形状为(批量大小, 通道数, 高, 宽)、值域在0到1之间且类型为32位浮点数。

```python
flip_aug = torchvision.transforms.Compose([
  torchvision.transforms.RandomHorizontalFlip(),
  torchvision.transforms.ToTensor()])

no_aug = torchvision.transforms.Compose([
  torchvision.transforms.ToTensor()])
```



#### 模型微调

应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。

本节我们介绍迁移学习中的一种常用技术：微调（fine tuning）。微调由以下4步构成：

1. 在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。
2. 创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。
3. 为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。
4. 在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。

当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。

##### 迁移学习

将源模型的输出层改成输出大小为目标数据集类别个数的输出层，对于这个新的输出层，随机初始化参数；在训练过程中，对输出层使用较大的学习率，对其它层使用较小的学习率（fine tuning）。