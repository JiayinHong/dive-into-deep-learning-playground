### Dive into deep learning - Part IV

------

#### 目标检测基础

##### Object Detection

目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框（ground-truth bounding box）。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）。我们将在后面基于锚框实践目标检测。

##### 交并比

我们刚刚提到某个锚框较好地覆盖了图像中的狗。如果该目标的真实边界框已知，这里的“较好”该如何量化呢？一种直观的方法是衡量锚框和真实边界框之间的相似度。我们知道，Jaccard系数（Jaccard index）可以衡量两个集合的相似度。给定集合A和B，它们的Jaccard系数即二者交集大小除以二者并集大小：
$$
J(A,B)=\frac{|A\cap B|}{|A\cup B|}
$$
实际上，我们可以把边界框内的像素区域看成是像素的集合。如此一来，我们可以用两个边界框的像素集合的Jaccard系数衡量这两个边界框的相似度。当衡量两个边界框的相似度时，我们通常将Jaccard系数称为交并比（Intersection over Union，IoU），即两个边界框相交面积与相并面积之比。交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框相等。

##### 标注训练集的锚框

在训练集中，我们将每个锚框视为一个训练样本。为了训练目标检测模型，我们需要为每个锚框标注两类标签：一是锚框所含目标的类别，简称类别；二是真实边界框相对锚框的偏移量，简称偏移量（offset）。在目标检测时，我们首先生成多个锚框，然后为每个锚框预测类别以及偏移量，接着根据预测的偏移量调整锚框位置从而得到预测边界框，最后筛选需要输出的预测边界框。

##### 输出预测边界框

在模型预测阶段，我们先为图像生成多个锚框，并为这些锚框一一预测类别和偏移量。随后，我们根据锚框及其预测偏移量得到预测边界框。当锚框数量较多时，同一个目标上可能会输出较多相似的预测边界框。为了使结果更加简洁，我们可以移除相似的预测边界框。常用的方法叫作非极大值抑制（non-maximum suppression，NMS）。

我们来描述一下非极大值抑制的工作原理。对于一个预测边界框B，模型会计算各个类别的预测概率。设其中最大的预测概率为p，该概率所对应的类别即B的预测类别。我们也将p称为预测边界框B的置信度。在同一图像上，我们将预测类别非背景的预测边界框按置信度从高到低排序，得到列表L。从L中选取置信度最高的预测边界框B1作为基准，将所有与B1的交并比大于某阈值的非基准预测边界框从L中移除。这里的阈值是预先设定的超参数。此时，L保留了置信度最高的预测边界框并移除了与其相似的其他预测边界框。 接下来，从L中选取置信度第二高的预测边界框B2作为基准，将所有与B2的交并比大于某阈值的非基准预测边界框从L中移除。重复这一过程，直到L中所有的预测边界框都曾作为基准。此时L中任意一对预测边界框的交并比都小于阈值。最终，输出列表L中的所有预测边界框。

实践中，我们可以在执行非极大值抑制前将置信度较低的预测边界框移除，从而减小非极大值抑制的计算量。我们还可以筛选非极大值抑制的输出，例如，只保留其中置信度较高的结果作为最终输出。

##### 小结

- 以每个像素为中心，生成多个大小和宽高比不同的锚框
- 交并比是两个边界框相交面积与相并面积之比
- 在训练集中，为每个锚框标注两类标签：一是锚框所含目标的类别；二是真实边界框相对锚框的偏移量
- 预测时，可以使用非极大值抑制来移除相似的预测边界框，从而令结果简洁

##### 多尺度目标检测

减少锚框个数并不难。一种简单的方法是在输入图像中均匀采样一小部分像素，并以采样的像素为中心生成锚框。此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。值得注意的是，较小目标比较大目标在图像上出现位置的可能性更多。举个简单的例子：形状为1×1、1×2和2×2的目标在形状为2×2的图像上可能出现的位置分别有4、2和1种。因此，当使用较小锚框来检测较小目标时，我们可以采样较多的区域；而当使用较大锚框来检测较大目标时，我们可以采样较少的区域。

##### 习题总结：

我们一般通过锚框中心像素xy坐标、锚框大小和宽高比来生成一组锚框



#### 图像风格迁移

##### 样式迁移（style transfer）

使用卷积神经网络自动将某图像中的样式应用在另一图像之上。两张输入图像，一张内容，另一张样式，使用神经网络修改内容图像使其在样式上接近样式图像。

##### 方法

用一个例子来阐述基于卷积神经网络的样式迁移方法。首先，我们初始化合成图像，例如将其初始化成内容图像。该合成图像是样式迁移过程中唯一需要更新的变量，即样式迁移所需迭代的模型参数。然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。深度卷积神经网络凭借多个层逐级抽取图像的特征。我们可以选择其中某些层的输出作为内容特征或样式特征。以图9.13为例，这里选取的预训练的神经网络含有3个卷积层，其中第二层输出图像的内容特征，而第一层和第三层的输出被作为图像的样式特征。接下来，我们通过正向传播（实线箭头方向）计算样式迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。样式迁移常用的损失函数由3部分组成：内容损失（content loss）使合成图像与内容图像在内容特征上接近，样式损失（style loss）令合成图像与样式图像在样式特征上接近，而总变差损失（total variation loss）则有助于减少合成图像中的噪点。最后，当模型训练结束时，我们输出样式迁移的模型参数，即得到最终的合成图像。

##### 预处理和后处理图像

预处理函数`preprocess`对输入图像在RGB三个通道分别做标准化，并将结果变换成卷积神经网络接受的输入格式。后处理函数`postprocess`则将输出图像中的像素值还原回标准化之前的值。由于图像打印函数要求每个像素的浮点数值在0到1之间，我们使用`clamp`函数对小于0和大于1的值分别取0和1。

##### 抽取特征

为了抽取图像的内容特征和样式特征，我们可以选择VGG网络中某些层的输出。一般来说，越靠近输入层的输出越容易抽取图像的细节信息，反之则越容易抽取图像的全局信息。为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，也称内容层，来输出图像的内容特征。我们还从VGG中选择不同层的输出来匹配局部和全局的样式，这些层也叫样式层。在[“使用重复元素的网络（VGG）”](https://www.kesci.com/api/notebooks/chapter_convolutional-neural-networks/vgg.ipynb)一节中我们曾介绍过，VGG网络使用了5个卷积块。实验中，我们选择第四卷积块的最后一个卷积层作为内容层，以及每个卷积块的第一个卷积层作为样式层。这些层的索引可以通过打印`pretrained_net`实例来获取。

##### 定义损失函数

样式迁移的损失函数由内容损失、样式损失和总变差损失3部分组成。

- 内容损失

  - 与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。平方误差函数的两个输入均为`extract_features`函数计算所得到的内容层的输出。

- 样式损失

  - 样式损失也一样通过平方误差函数衡量合成图像与样式图像在样式上的差异。样式损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与样式图像的样式层输出。

- 总变差损失

  - 有时我们学到的合成图像里面有大量高频噪点，即有特别亮或特别暗的颗粒像素。一种常用的降噪方法是总变差降噪（total variation denoising）。假设$x_{i,j}$表示坐标为$(i,j)$的像素值，降低总变差损失
    $$
    \sum_{i,j}{|x_{i,j}-x_{i+1,j}|+|x_{i,j}-x_{i,j+1}|}
    $$
    能够尽可能使临近的像素值相似。

- 损失函数

  - 样式迁移的损失函数即内容损失、样式损失和总变差损失的加权和。通过调节这些权值超参数，我们可以权衡合成图像在保留内容、迁移样式以及降噪三方面的相对重要性。

##### 创建和初始化合成图像

在样式迁移中，合成图像是唯一需要更新的变量。因此，我们可以定义一个简单的模型`GeneratedImage`，并将合成图像视为模型参数。模型的前向计算只需返回模型参数即可。

##### 训练

在训练模型时，我们不断抽取合成图像的内容特征和样式特征，并计算损失函数。

##### 小结

- 样式迁移常用的损失函数由3部分组成：内容损失使合成图像与内容图像在内容特征上接近，样式损失令合成图像与样式图像在样式特征上接近，而总变差损失则有助于减少合成图像中的噪点。
- 可以通过预训练的卷积神经网络来抽取图像特征，并通过最小化损失函数来不断更新合成图像。
- 用格拉姆矩阵表达样式层输出的样式。

##### 习题总结：

1. 关于特征抽取

   - 通过选取靠近输入的层来抽取样式特征
   - 我们不改变网络模型参数，只对合成图像的内容进行训练更新
   - 我们通过逐层计算来抽取输入图像的特征
   - 使用VGG网络各个卷积块的第一层作为样式层

2. 关于损失函数

   - 用Gram矩阵衡量各个通道上的样式特征的相关性
   - 先求得Gram矩阵，用两个Gram矩阵的平方误差衡量样式损失
   - Gram矩阵尺寸仅与通道数有关，与原图像的高和宽无关

3. 关于训练过程

   - 训练完成后，合成图像就是最终结果，不需要再经过网络卷积块的计算
   - 合成图像应该与内容图像、样式图像具有相同的形状
   - 通过修改合成图像的内容来减小损失值
   - 内容图像和样式图像的特征只需要提取一次：内容图像和样式图像的特征在整个训练过程中是不变的，在训练前预先计算好即可

4. 在图像分类任务中，缓解过拟合的手段包括：

   - 图像增广
   - 权重衰减
   - 增加训练样本

   而从原训练数据集切分出验证数据集并不能缓解过拟合，其作用是用于调整超参数

5. 关于`ImageFolder`类：

   - 创建`ImageFolder`实例时，它会将给定的路径参数`root`下每个文件目录都当作一种类别
   - 创建`ImageFolder`实例时，可以传入参数`transform`，`transform`应当是可调用的（callable），取出一个样本时，会用`transform`先对其进行处理
   - 创建`ImageFolder`实例时，`ImageFolder`维护样本的顺序是确定的：以类别目录名为第一关键字，类别目录下的图像文件名为第二关键字，按字典序从小到大依次处理
   - 创建`ImageFolder`实例时，`ImageFolder`会维护一部分可接受的图像文件类型，若某个类别的目录下存在一个文本文件，对于不可接受的文件类型会直接忽略掉

6. 关于微调ResNet-34预训练模型进行图像分类：

   - 图像的类别发生变化，需要替换输出层
   - 由于我们不希望改变模型的特征提取部分的参数，所以可以对该部分参数设置`requires_grad = False`来冻结这些参数，使它们不参与训练
   - 定义优化器时，因为只需要训练输出层部分的模型参数，所以只需传入输出层部分的模型参数



#### 生成对抗网络（Generative Adversarial Networks，GAN）

##### 小结

- 生成对抗网络由两个深度网络组成，the generator and the discriminator
- 生成器（generator）合成与真实图像尽可能相近的图像来“迷惑”分类器（discriminator），通过最大化cross-entropy loss
- discriminator则尽量区分生成图像和真实图像，通过最小化cross-entropy loss

##### 习题总结

1. 关于生成对抗神经网络
   - 网络结构中包含生成器和判别器两个网络模型
   - 判别器是一个分类模型
   - 根据不同的神经网络结构，GAN可以被用来生成多种不同的数据类型
   - 训练过程中，生成器和判别器的参数都会进行更新和训练
2. 关于GAN的细节描述
   - 判别器的输入包括数据集中的真实数据和生成器生成的数据
   - 生成对抗神经网络的思想是让生成器能生成以假乱真的数据
   - 实际训练中判别器使用的损失函数是 $\min_D\{-y\log{D(\mathbf{x})}-(1-y)\log(1-D(\mathbf{x}))\}$
   - 为了避免梯度消失问题，我们对生成器的损失函数进行了一些更改：实际训练中生成器使用的损失函数是 $\min_G\{-y\log(D(G(\mathbf{z})))\}$



#### DCGAN(Deep Convolutional Generative Adversarial Networks)

##### 深度卷积生成对抗网络

##### 小结

- DCGAN architecture has four convolutional layers for the Discriminator and four "fractionally-strided" convolutional layers for the Generator.
- The Discriminator is a 4-layer strided convolutions with batch normalization (except its input layer) and leaky ReLU activations.
- Leaky ReLU is a nonlinear function that give a non-zero output for a negative input. It aims to fix the “dying ReLU” problem and helps the gradients flow easier through the architecture.
- 转置卷积层能将输出的size放大：If changing the transposed convolution layer to a 4×4 kernel, 1×1 strides and zero padding. With a input size of 1×1, the output will have its width and height increased by 3 respectively.

##### 习题总结

1. 假设数据使用`ToTensor()`进行处理，generator在输出前使用`tanh`函数激活，那么我们还需要对数据做均值为0.5，标准差为0.5对归一化处理

   - `ToTensor()`输出值在[0,1]区间，`tanh`函数输出区间在[-1,1]

2. $16\times16$的输入，过一个卷积核大小为$4\times4$，步长为2，padding为1的转置卷积层后，输出的大小是$32\times32$

   - In default, the transposed convolution layer uses a $k_h = k_w = 4$ kernel, a $s_h = s_w = 2$ strides, and a $p_h = p_w = 1$ padding. With an input shape of $n_h\times n_w = 16\times16$, the generator block will double input's width and height.
     $$
     \begin{align}
     n_h'\times n_w' &=[n_hk_h-(n_h-1)(k_h-s_h)-2p_h]\times[n_wk_w-(n_w-1)(k_w-s_w)-2p_w]\\
     &=(16*4-15*2-2)*(16*4-15*2-2)\\
     &=32\times32
     \end{align}
     $$



